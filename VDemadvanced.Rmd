---
title: "Vdem"
author: "Lucca"
date: "2025-01-20"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This code is from of a multiple imputation procedure I did as part of a statistics capstone project with a government agency. The goal was to use VDem's de facto information provided by raters to create visualizations and new classifications of countries' democratic status. If replicating, don't forget to install the vdem package from github, mice, and tidyverse:

```{r}
#devtools::install_github("vdeminstitute/vdemdata")
library(tidyverse)
library(vdemdata)

#read in dataset, using fill_vars to carry foward election specific variables. fill_vars still leaves na values for breaks in electoral regime for election variables. During these times, these variables should be set to a theoretical minimum, but that's not clearly defined when using the Bayesian model latent estimates. Here, we impute these values along with other missing vars.

vdemcsv<-fill_vars()

#rescaleFreedomHouse variable 

vdemcsv$rescaled<-(as.numeric(vdemcsv$e_fh_pr)+as.numeric(vdemcsv$e_fh_cl))/2

#filter out irrelevant variable versions, only interested in Bayesian latent model estimates.

vdemcsv<-vdemcsv[,!(grepl("_osp|_ord|_codelow|_codehigh|_mean|v2x|v3|_sd|_mean",names(vdemcsv)))] 

#isolate number of coders and create NAs for variables without minimum number of coders

coders<-vdemcsv[,grepl("_nr",names(vdemcsv))]
a<-data.frame(rown=1:length(vdemcsv$country_name))
for(i in 1:length(names(coders))){
vect<-pull(coders[,i])
vect<-ifelse(is.na(vect),0,vect)


a[,i]<-as.logical(vect>=1) #Sets minimum number of coders allowable.

names(a)[i]<-substr(names(coders)[i],1,nchar(names(coders)[i])-3)
}

#every variable listed in falseindf has a naming scheme that in some way does not align with the conventions of the rest of the Vdem naming scheme. This is generally because the larger variable is composed of many binary variables. While these dichotomous variables are important, individually they add little information, and they are not intuitive or interesting to visualize. Here we scrap them. 

d<-c()
for(i in 1:length(names(a))){
d[i]<-names(a)[i]%in%names(vdemcsv)
}
false_indices <- which(!d)
falseindf<-names(a)[false_indices]
a<-a[,-false_indices]

#We also do not want to include text variables, or variables whose numerical response is actually indicative of a factor referenced in the codebook. Ex. A response of 5 means "The Military" rather than 5. In effect, we want variables whose responses are either in interval form or converted to interval by the measurement model. 

vartype<-c()
for(i in 1:length(names(a))){
vartype[i]<-var_info(names(a)[i])[13]
}
vartype<-unlist(vartype)
vartype<-tolower(vartype)
a<-a[,grepl("interval",vartype)]

#Some NAs are meant to be set to NA because another variable in the dataset has a certain value. In the Vdem codebook, this information is contained in cleaning details in the format "Set to missing where v2regint is 0." but there is inconsistent language, grammar, and punctuation used in the codebook. We want to clean the codebook entries to isolate the variable name that needs to be checked (eg. v2regint in the above example) and the value that it needs to be checked for (eg. 0). Here I handle this with a regular expression.

ordering<-c()
cleaning<-c()
for(i in 1:length(names(a))){
cleaning[i]<-var_info(names(a)[i])[36]
}
cleaning<-unlist(cleaning)

contingent<-rep(NA,length.out=length(cleaning))
v_index<-c()
  v_index[i]<- regexpr("v", cleaning[i], ignore.case = TRUE)

for (i in 1:length(cleaning)) {
  v_index<- regexpr("v", cleaning[i], ignore.case = TRUE)
  if (!is.na(v_index)) {
    space_index <- regexpr("\\s", substr(cleaning[i], v_index, nchar(cleaning[i])))
    if (is.na(space_index)) {
      contingent[i] <- substr(cleaning[i], v_index, nchar(cleaning[i]))
    } else {
      contingent[i] <- substr(cleaning[i], v_index, v_index + space_index - 2)
    }
  }
}


checkvalue<- as.numeric(gsub("[^0-9]", "", substr(cleaning, nchar(cleaning) - 2, nchar(cleaning))))

truenas<-data.frame(column=names(a),variable=contingent, value=checkvalue)
truenas<-truenas%>%filter(!is.na(variable))%>%filter(!(column=="juhcind"|column=="juhccomp"))

#truenas is a dataset that tells you which columns in the dataframe it should default to NA when "variable"="value"
#However, reviewing  the cleaning rules though we see that some columns have rules that our regular expression did not capture. 
cleaning
names(a)[226]

#v2edscpatriotcb is set to 0 not NA when v2edscpatriot is 0, so those values do not need imputation
truenas<-truenas[!truenas$column=="v2edscpatriotcb",]
names(a)[221]
names(a)[222]
cleaning[221]

#v2medpolstate v2medpolnonstate are dependent on v2medstateprint and v2medstatebroad to BOTH be set to a value in order to render a missing by design. To handle this we create indicator variables.

medpolstatedummy<-ifelse((vdemcsv$v2medstatebroad+vdemcsv$v2medstateprint)==0,0,1)
medpolnonstatedummy<-ifelse((vdemcsv$v2medstatebroad+vdemcsv$v2medstateprint)==8,4,0)
truenas[truenas$column%in%c("v2medpolstate"),2]<-"medpolstatedummy"
truenas[truenas$column%in%c("v2medpolnonstate"),2]<-"medpolnonstatedummy"

#v2lgdomchm and are set to missing when v2lgbicam is 0 OR 1. So we need to create dummy for that as well.

lgbicamdummy<-ifelse(vdemcsv$v2lgbicam==0|vdemcsv$v2lgbicam==1,1,0)
truenas[truenas$column%in%c("v2lgdomchm","v2lglegpup"),2]<-"lgbicamdummy"
```

Now we want to reconstruct the Vdem dataset, with NAs for variables with insufficient coders and only the columns of interest and country-year combos. 

```{r}
#We add back in population and gdp per capita as controls for our imputation process. Assumption is that data is missing at random after controlling for these factors. This is probably a faulty assumption given what we know about the Vdem.

slim<-data.frame(e_pop=vdemcsv$e_pop,e_gdppc=vdemcsv$e_gdppc,rescaled=vdemcsv$rescaled,year=vdemcsv$year)
for(i in 1:length(names(a))){
ayy<-pull(vdemcsv[,names(vdemcsv)%in%names(a)[i]])
Bee<-a[,i]
vdemcsv[,names(vdemcsv)==names(a)[i]]<-ifelse(Bee==FALSE,NA,ayy)
}
slimlength<-length(slim)
for(i in 1:length(names(a))){
slim[,i+slimlength]<-pull(vdemcsv[,names(vdemcsv)%in%names(a)[i]])
names(slim)[i+slimlength]<-names(a)[i]
}

#filter for after 1972. That's when freedomhouse designations started. 

slim<-slim[slim$year>=1972,]
vdemcsv<-vdemcsv[vdemcsv$year>=1972,]
vdemcsv$v2elffelrbin_ord<-vdem$v2elffelr_ord[vdem$year>=1972]
vdemcsv$medpolstatedummy<-medpolstatedummy[vdem$year>=1972]
vdemcsv$medpolnonstatedummy<-medpolnonstatedummy[vdem$year>=1972]
vdemcsv$lgbicamdummy<-lgbicamdummy[vdem$year>=1972]

#removing variables with convergence issues for country date latent trait estimates v2smgovsmalt v2smforads v2caconmob v2peapssoc

badvariables<-c("v2smgovsmalt","v2smforads", "v2caconmob", "v2peapssoc")
slim<-slim[,!(names(slim)%in%badvariables)]

#need to create a dataframe for the where" argument in mice so we don't impute for NAs by design.

wheredf<-is.na(slim)
for(i in 1:length(truenas)){
vectora<-drop(wheredf[,colnames(wheredf)%in%truenas$column[i]])
vectorb<-pull(vdemcsv[,names(vdemcsv)%in%truenas$variable[i]])
vectorc<-ifelse(is.na(vectorb),TRUE,ifelse(vectorb==truenas$value[i],FALSE,vectora))
wheredf[,names(slim)%in%truenas$column[i]]<-vectorc
}

```

```{r}
#eliminating variables with more than 1000 NAs.
notapplic<-data.frame(variablename=colnames(wheredf),numberofNAs=apply(wheredf,2,function(x) sum((x))))
wheredf<-wheredf[,!notapplic$numberofNAs>1000]
slim<-slim[,notapplic$numberofNAs<=1000]
sapply(slim[,(1+slimlength):length(slim)],function(x) min(x,na.rm=T))

#created a list with maximum and minimum bounds for the imputation process
boundsdf1<-data.frame(names=names(slim)[(1+slimlength):length(slim)],min=rep(-5,length.out=(length(slim)-slimlength)),max=rep(5,length.out=(length(slim)-slimlength)))
exceptions<-ifelse(sapply(slim[,(1+slimlength):length(slim)],function(x) max(x,na.rm=T))>5,TRUE,FALSE)
boundsdf1$max<-ifelse(exceptions==TRUE,100,5)
boundsdf1$min<-ifelse(exceptions==TRUE,0,-5)
boundsdfpivot<-t(boundsdf1[,2:3])
colnames(boundsdfpivot)<-boundsdf1$names
boundsdfpivot<-data.frame(boundsdfpivot)
boundsdf<-as.list(boundsdfpivot)
```
Begin imputation, be aware it takes about 30 minutes
```{r}
#imputation 
library(mice)
library(miceadds)
#Any character variables used in MICE must be made factors first. 
imputationstation<- mice(slim, m=5, maxit = 5,where=wheredf, bounds=boundsdf,print =  TRUE)
#save the imputed dataframes as a long form csv
newimpute <- complete(imputationstation, action="long", include = TRUE)
#when reloading saved longform data, use as.mids() to convert back to multiple imputed dataset format for analysis

```

#eliminating variables with more than 1000 NAs
notapplic<-data.frame(variablename=names(sapply(slim,function(x) sum(is.na(x)))),numberofNAs=sapply(slim,function(x) sum(is.na(x))))
slim<-slim[,!notapplic$numberofNAs>1000]

check<-apply(slimrecent,1,function(x) sum(is.na(x)))
slimrecent$totalNAs<-check
NAbycountry<-slimrecent[ ,names(slimrecent)=="country_name"]
NAyear<-slimrecent[ ,names(slimrecent)=="year"]
NAbycountry<-cbind(NAbycountry,NAyear,check)
aggregated<-aggregate(check~country_name,data=NAbycountry,FUN=mean)
?sapply
slimmer<-slimrecent[,24:length(names(slimrecent))]
newnotapplic<-data.frame(variablename=names(sapply(newslim,function(x) sum(is.na(x)))),numberofNAs=sapply(newslim,function(x) sum(is.na(x))))
slimmest<-newslim[,names(newslim)[newnotapplic$numberofNAs<1000]]
head(data.new)
questions<-c(0)
for(i in 1:length(names(slimmest))){
questions[i]<-var_info(names(slimmest)[1:i])[9]
}
vdem_codebook
infocheck<-var_info("v2elrgstry")
?fill_vars
f<-data.frame(a=unlist(infocheck))

f$row<-row(f)



```{r}
#removing highly correlated
cormatrix<-cor(apply(slimrecent[,24:length(names(slimrecent))],2,FUN=function(x) as.numeric(x)),use="pairwise.complete.obs")
cormatrix[upper.tri(cormatrix)]<-0
diag(cormatrix)<-0
newslim <- slimmer[, !apply(cormatrix, 2, function(x) any(abs(x) > .7, na.rm = TRUE))]

```

names(vdemcsv)
getOption("max.print")
options(max.print(5000)))
